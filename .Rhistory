vol_qry(
id = id,
from = to_iso8601(latestData, -4),
to = to_iso8601(latestData, 0)
) %>%
GQL(., .url = configs$vegvesen_url) %>%
transform_volumes() %>%
ggplot(aes(x=from, y=volume)) +
geom_line() +
theme_classic()
CosineSimilarity <- function(A, B){
((A[1] * B[1]) + (A[2] * B[2])) / sqrt((A[1]^2 + A[2]^2) * (B[1]^2 + B[2]^2))
}
CosineSimilarity((1,0), (0, 1))
CosineSimilarity((1, 0), (0, 1))
CosineSimilarity(c(1, 0), c(0, 1))
CosineSimilarity(c(1, 0), c(1, 0))
CosineSimilarity(c(2, 0), c(1, 0))
CosineSimilarity(c(3, 0), c(1, 0))
CosineSimilarity(c(3, 2), c(1, 0))
CosineSimilarity(c(1, 0), c(1, 0))
CosineSimilarity <- function(A, B){
((A[1] * B[1]) + (A[2] * B[2])) /
sqrt((A[1]^2 + A[2]^2) * (B[1]^2 + B[2]^2))
}
CosineSimilarity(c(1, 0), c(1, 0))
CosineSimilarity <- function(A, B){
((A[1] * B[1]) + (A[2] * B[2]))
/
CosineSimilarity(c(1, 0), c(1, 0))
CosineSimilarity(c(0, 1), c(1, 0))
require(tm)
require(slam)
require(tidyverse)
setwd("~/Documents/NHH/4/BAN432 Applied Textual Data Analysis for Business and Finance/Lecture 13")
load("firm_dataset.Rda")
load("firm_dataset.Rda")
load("firm_dataset.Rdata")
load("firm_dataset.Rdata")
A = c(1, 0, 0)
B = c(0, 1, 1)
CosineSimilarity <- function(A, B) {
sum(A * B) / sqrt(sum(A^2) * sum(B^2))
}
CosineSmilarity(A, B)
CosineSimilarity(A, B)
A = c(1, 1, 0)
CosineSimilarity(A, B)
# Text corpus to dtm
corpus <- Corpus(VectorSource(section.1.business))
dtm <- DocumentTermMatrix(corpus,
control = list(
bounds = list( global =
c(.01, .10)* length)))
dtm <- DocumentTermMatrix(corpus,
control = list(
bounds = list( global =
c(.01, .10) * length)))
dtm <- DocumentTermMatrix(corpus,
control = list(
bounds = list(
global = c(.01, .10) * length))))
dtm <- DocumentTermMatrix(corpus,
control = list(
bounds = list(
global = c(.01, .10) * length)))
dtm <- DocumentTermMatrix(corpus,
control = list(
bounds = list(
global = c(.01, .10) * length(section.1.business))))
View(dtm)
View(dtm)
# Make binary
dtm$v <- rep(1, length(dtm$v))
mcdonald.cik <- "0000063908"
cocacola.cik <- "0001491675"
pepsi.cik <- "0000077476"
CosineSimilarity(A = dtm[raw.data$cik == mcdonald.cik])
CosineSimilarity(B = dtm[raw.data$cik == cocacola.cik])
CosineSimilarity(A = dtm[raw.data$cik == mcdonald.cik])
CosineSimilarity((A = dtm[raw.data$cik == mcdonald.cik]), (B = dtm[raw.data$cik == cocacola.cik]))
CosineSimilarity(A = dtm[raw.data$cik == mcdonald.cik],
B = dtm[raw.data$cik == cocacola.cik])
View(dtm)
CosineSimilarity(A = dtm[raw.data$cik == mcdonald.cik],
B = dtm[raw.data$cik == cocacola.cik])
mcdonald.cik <- "0000063908"
raw.data %>% filter(cik == mcdonald.cik) %>%
select(industry.fama.french.49) %>% unlist()
cocacola.cik <- "0001491675"
raw.data %>% filter(cik == cocacola.cik) %>%
select(industry.fama.french.49) %>% unlist()
pepsi.cik <- "0000077476"
raw.data %>% filter(cik == pepsi.cik) %>%
select(industry.fama.french.49) %>% unlist()
CosineSimilarity(A = dtm[raw.data$cik == mcdonald.cik],
B = dtm[raw.data$cik == cocacola.cik])
clusters <- kmeans(x = dtm,
centers = 50)
clusters&clusters
clusters$cluster
class(clusters)
ls(clusters)
raw.data$industry.kmeans <- clusters$cluster
raw.data <-
# Define variable
mutate(var = operating.income / total.assets) %>%
# define which industry
group_by(industry.kmeans) %>%
# cimpute average per industry
summarise(varIndustryMean = mean(var))
raw.data <-
# Define variable
mutate(var = operating.income / total.assets) %>%
# define which industry
group_by(industry.kmeans) %>%
# compute average per industry
summarise(varIndustryMean = mean(var))
raw.data %>%
# Define variable
mutate(var = operating.income / total.assets) %>%
# define which industry
group_by(industry.kmeans) %>%
# compute average per industry
summarise(varIndustryMean = mean(var))
# standard deviation between each industry
pull(varIndustryMean) %>% sd()
# kmeans
set.seed(1234)
clusters <- kmeans(x = dtm,
centers = 50)
raw.data$industry.kmeans <- clusters$cluster
raw.data %>%
# Define variable
mutate(var = operating.income / total.assets) %>%
# define which industry
group_by(industry.kmeans) %>%
# compute average per industry
summarise(varIndustryMean = mean(var))
# standard deviation between each industry
pull(varIndustryMean) %>% sd()
raw.data %>%
# Define variable
mutate(var = operating.income / total.assets) %>%
# define which industry
group_by(industry.kmeans) %>%
# compute average per industry
summarise(varIndustryMean = mean(var)) %>%
# standard deviation between each industry
pull(varIndustryMean) %>% sd()
CosineSimilarity(A = dtm[raw.data$cik == mcdonald.cik],
B = dtm[raw.data$cik == cocacola.cik])
CosineSimilarity(A = dtm[raw.data$cik == mcdonald.cik,],
B = dtm[raw.data$cik == cocacola.cik,])
View(raw.data)
View(raw.data)
require(SentimentAnalysis)
require(sentimentr)
load("firm_dataset.Rdata")
View(raw.data)
View(raw.data)
section.7.mda %>%
VectorSource() %>%
Corpus() %>%
DocumentTermMatrix() -> dtm
section.7.mda %>%
VectorSource() %>%
Corpus() %>%
DocumentTermMatrix(corpus,
control = list(stopwords = T
)) -> dtm
section.7.mda %>%
VectorSource() %>%
Corpus() %>% -> corpus
section.7.mda %>%
VectorSource() %>%
Corpus() -> corpus
remove(dtm)
dtm <- DocumentTermMatrix(corpus,
control = list(stopwords = T
))
View(dtm)
View(dtm)
View(dtm)
View(dtm)
require(tidyverse)
tidy <- tidy(dtm)
require(broom)
tidy <- tidy(dtm)
require(tm)
tidy <- tidy(dtm)
tidy <- tidy(dtm)
require(tm)
library(tm)
require(sentimentr)
require(tm)
tidy <- tidy(dtm)
termFreq <- tibble(term = colnames(dtm),
freq = colsums(dtm))
termFreq <- tibble(term = colnames(dtm),
freq = col_sums(dtm))
View(termFreq)
View(termFreq)
View(termFreq)
View(termFreq)
tremFreq %>%
arrange(-freq) -> termFreq
termFreq %>%
arrange(-freq) -> termFreq
termFreq %>%
arrange(-freq) %>%
filter(term %in% DictionaryLM$negative) -> termFreq
View(termFreq)
View(termFreq)
View(termFreq)
View(termFreq)
View(raw.data)
View(raw.data)
raw.data$fin.neg <- row_sums(dtm[ , colnames(dtm) %in% DictionaryLM$negative]) / row_sums(dtm)
View(raw.data)
View(raw.data)
View(corpus)
View(corpus)
sentiments <- sentiment_by(corpus, by = "sentence")
text <- sapply(corpus, as.character)
sentiments <- sentiment_by(text, by = "sentence")
sentences <- get_sentences(text)
sentiments <- sentiment_by(sentences)
View(sentiments)
View(sentiments)
View(sentences)
View(sentiments)
View(corpus)
View(corpus)
a <- sentiment(sentences)
View(a)
?sentiment()
View(raw.data)
View(raw.data)
annual_reports <- load("firm_dataset.Rdata")
sentiments <- annual_reports %>%
mutate(sentiment = sentiment(text))
library(dplyr)
sentiments <- annual_reports %>%
mutate(sentiment = sentiment(text))
corpus <- section.7.mda %>%
VectorSource() %>%
Corpus()
sentiments <- annual_reports %>%
mutate(sentiment = sentiment(corpus))
sentiments <- annual_reports %>%
mutate(sentiment = sentiment(corpus))
sentiments <- annual_reports %>%
mutate(sentiment = sentiment(corpus))
library(ggplot2)
library(ggthemes)
library(tibble)
library(dplyr)
library(SentimentAnalysis)
library(tm)
library(slam)
# Load your dataset
load("firm_dataset.Rdata")
# Extract the text data from the R environment
section_7_mda <- get("section.7.mda")
View(raw.data)
View(raw.data)
insurance_reports <- section.7.mda[raw.data$industry.fama.french.49 == "46 Insur"]
non_insurance_reports <- section.7.mda[raw.data$industry.fama.french.49 != "46 Insur"]
# Perform sentiment analysis on insurance reports
sentiment_insurance <- analyzeSentiment(insurance_reports)$SentimentLM
# Perform sentiment analysis on non-insurance reports
sentiment_non_insurance <- analyzeSentiment(non_insurance_reports)$SentimentLM
# Calculate average sentiment scores for insurance and non-insurance firms
average_sentiment_insurance <- mean(sentiment_insurance)
average_sentiment_non_insurance <- mean(sentiment_non_insurance)
# Print the average sentiment scores
cat("Average sentiment for insurance firms:", average_sentiment_insurance, "\n")
cat("Average sentiment for non-insurance firms:", average_sentiment_non_insurance, "\n")
# Print the average sentiment scores
print("Average sentiment for insurance firms:", average_sentiment_insurance)
# Print the average sentiment scores
print("Average sentiment for insurance firms:" + average_sentiment_insurance)
# Print the average sentiment scores
cat("Average sentiment for insurance firms:", average_sentiment_insurance, "\n")
print("Average sentiment for insurance firms:", average_sentiment_insurance)
# Load your dataset
load("firm_dataset.Rdata")
corpus <- section.7.mda %>%
VectorSource() %>%
Corpus()
corpus <- section.7.mda %>%
VectorSource() %>%
Corpus()
dtm <- DocumentTermMatrix(corpus,
control = list(stopwords = T,
))
dtm <- DocumentTermMatrix(corpus,
control = list(stopwords = T))
View(corpus)
View(corpus)
View(dtm)
View(dtm)
dtm <- DocumentTermMatrix(corpus,
control = list(stopwords = T)) %>%
tidy()
tidy(dtm)
tidy(dtm)
library(slam)
?tidy
?tidy
library(tidytext)
tidy(dtm)
tidy(dtm) -> x
View(x)
View(x)
View(dtm)
View(dtm)
termFreq <- tibble(term = colnames(dtm),
freq = col_sums(dtm))
View(termFreq)
View(termFreq)
View(termFreq)
View(termFreq)
termFreq %>%
arrange(-freq) %>%
filter(term %in% DictionaryLM$negative) -> termFreq
View(termFreq)
View(termFreq)
termFreq %>%
arrange(-freq) %>%
filter(term %in% SentimentLM$negative) -> termFreq
termFreq %>%
arrange(-freq) %>%
filter(term %in% DictionaryLM$negative) -> termFreq
insurance_corpus <- insurance_reports %>%
VectorSource() %>%
Corpus()
insurance_reports <- section.7.mda[raw.data$industry.fama.french.49 == "46 Insur"]
non_insurance_reports <- section.7.mda[raw.data$industry.fama.french.49 != "46 Insur"]
sentiment_insurance <- analyzeSentiment(insurance_reports)$SentimentLM
sentiment_non_insurance <- analyzeSentiment(non_insurance_reports)$SentimentLM
mean_sentiment_insurance <- mean(sentiment_insurance)
mean_sentiment_non_insurance <- mean(sentiment_non_insurance)
# Printing the average sentiment scores
cat("Average sentiment for insurance firms:", average_sentiment_insurance, "\n")
# Printing the average sentiment scores
cat("Average sentiment for insurance firms:", mean_sentiment_insurance, "\n")
cat("Average sentiment for non-insurance firms:", mean_sentiment_non_insurance, "\n")
insurance_corpus <- insurance_reports %>%
VectorSource() %>%
Corpus()
View(insurance_corpus)
View(insurance_corpus)
termFreq
# Printing the average sentiment scores
cat("Average sentiment for insurance firms:", mean_sentiment_insurance, "\n")
cat("Average sentiment for non-insurance firms:", mean_sentiment_non_insurance, "\n")
termFreq
library(ggplot2)
library(ggthemes)
library(tibble)
library(dplyr)
library(SentimentAnalysis)
library(tm)
library(slam)
# Load the dataset
load("firm_dataset.Rdata")
# Filtering out the insurance firms and the remaining ones
insurance_reports <- section.7.mda[raw.data$industry.fama.french.49 == "46 Insur"]
non_insurance_reports <- section.7.mda[raw.data$industry.fama.french.49 != "46 Insur"]
# Sentiment analysis on the insurance firms and the remaining ones
sentiment_insurance <- analyzeSentiment(insurance_reports)$SentimentLM
sentiment_non_insurance <- analyzeSentiment(non_insurance_reports)$SentimentLM
# Calculating the average sentiment scores
mean_sentiment_insurance <- mean(sentiment_insurance)
mean_sentiment_non_insurance <- mean(sentiment_non_insurance)
# Printing the average sentiment scores
cat("Average sentiment for insurance firms:", mean_sentiment_insurance, "\n")
cat("Average sentiment for non-insurance firms:", mean_sentiment_non_insurance, "\n")
insurance_corpus <- insurance_reports %>%
VectorSource() %>%
Corpus()
# Creating a dtm
dtm <- DocumentTermMatrix(corpus,
control = list(stopwords = T))
# Creating a corpus
insurance_corpus <- insurance_reports %>%
VectorSource() %>%
Corpus()
# Creating a dtm
dtm <- DocumentTermMatrix(corpus,
control = list(stopwords = T))
# Creating a dtm
dtm <- DocumentTermMatrix(insurance_corpus,
control = list(stopwords = T))
# Creating a tibble containing the word and their frequencies
termFreq <- tibble(term = colnames(dtm),
freq = col_sums(dtm))
# Arranging the tibble in descending order and filter such that only negative words
# from DictionaryLM is in the tibble
termFreq %>%
arrange(-freq) %>%
filter(term %in% DictionaryLM$negative) -> termFreq
termFreq
# clear workspace
rm(list =ls())
require(tm)
require(topicmodels)
require(slam)
require(wordcloud)
# load data
load("firm_dataset.Rda")
setwd("~/Documents/NHH/4/BAN432 Applied Textual Data Analysis for Business and Finance/Lecture 14")
# load data
load("firm_dataset.Rda")
# load data
load("firm_dataset.Rda")
# load data
load("firm_dataset.Rdata")
# make document-term-matrix
corpus <- Corpus(VectorSource(section.1.business))
dtm <- DocumentTermMatrix(corpus,
control = list(
removePunctuation = T,
stopwords = T,
stemming = F,
removeNumbers = T,
wordLengths = c(4, 20),
bounds = list(global = c(10,50))))
dtm <- dtm[row_sums(dtm) > 10,]
dim(dtm)
# estimate topic model
topic <- LDA(dtm,  # document term matrix
k = 25, # specifify number of topics
method = "Gibbs",
control = list(
seed = 1234, # eases replication
burnin = 100,  # how often sampled before estimation recorded
iter = 300,  # number of iterations
keep = 1,    # saves additional data per iteration (such as logLik)
save = F,     # saves logLiklihood of all iterations
verbose = 10  # report progress
))
# Loglikelihood of the choosen model
topic@loglikelihood
plot(topic@logLiks, type = "l")
# Controls defining the model
topic@k
topic@alpha
topic@control
topic@iter
# Term distribution for each topic
beta <- exp(topic@beta) # log of probability reported
dim(beta)
# inspect the most common terms of topic 1
head(topic@terms[order(beta[1,], decreasing = T)], 20)
# inspect top 5 terms for all topics
apply(topic@beta, 1, function(x) head(topic@terms[order(x, decreasing = T)],5))
library(httr)
library(jsonlite)
library(ggplot2)
library(DescTools)
library(tidyverse)
library(magrittr)
library(rlang)
library(lubridate)
library(anytime)
library(readr)
library(yaml)
# Load function for posting GQL-queries and retrieving data:
source("functions/GQL_function.r")
setwd("~/Documents/NHH/4/BAN400_R_programming/iterations-s232668")
# Load function for posting GQL-queries and retrieving data:
source("functions/GQL_function.r")
configs <-
read_yaml("vegvesen_configs.yml")
gql_metadata_qry <- read_file("gql-queries/station_metadata.gql")
stations_metadata <-
GQL(
query=gql_metadata_qry,
.url = configs$vegvesen_url
)
source("functions/data_transformations.r")
stations_metadata_df <-
stations_metadata %>%
transform_metadata_to_df(.)
stations_metadata_df <-
stations_metadata %>%
transform_metadata_to_df(.)
source("functions/data_transformations.r")
stations_metadata_df <-
stations_metadata %>%
transform_metadata_to_df(.)
# Load function for posting GQL-queries and retrieving data:
source("functions/GQL_function.r")
configs <-
read_yaml("vegvesen_configs.yml")
gql_metadata_qry <- read_file("gql-queries/station_metadata.gql")
stations_metadata <-
GQL(
query=gql_metadata_qry,
.url = configs$vegvesen_url
)
stations_metadata <-
GQL(
query=gql_metadata_qry,
.url = configs$vegvesen_url
)
?GQL
?GQL
?GQL()
